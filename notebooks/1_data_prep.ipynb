{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0543ea8d",
   "metadata": {},
   "source": [
    "# Task 1: Data Preparation\n",
    "\n",
    "This notebook implements the data preparation pipeline:\n",
    "1. Create/organize gallery dataset (20 identities, 5-10 images each)\n",
    "2. Face detection\n",
    "3. Face alignment (5-point landmarks)\n",
    "4. Normalization and cropping\n",
    "5. Train/validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d44c3",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "705eaffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gallery directory: ..\\data\\gallery\n",
      "Aligned output: ..\\data\\gallery_aligned\n",
      "Validation directory: ..\\data\\validation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "# Set paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "GALLERY_DIR = DATA_DIR / \"gallery\"\n",
    "ALIGNED_DIR = DATA_DIR / \"gallery_aligned\"\n",
    "VALIDATION_DIR = DATA_DIR / \"validation\"\n",
    "\n",
    "# Create directories\n",
    "ALIGNED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VALIDATION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Gallery directory: {GALLERY_DIR}\")\n",
    "print(f\"Aligned output: {ALIGNED_DIR}\")\n",
    "print(f\"Validation directory: {VALIDATION_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5019f30",
   "metadata": {},
   "source": [
    "## 2. Install and Load Face Detection Model\n",
    "\n",
    "We'll use **YOLO Face Detection** from Ultralytics for robust face detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d91a0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv8 nano model for face detection...\n",
      "âœ… Face detection model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 model for face detection\n",
    "# Using YOLOv8n (nano) for faster inference\n",
    "# It will auto-download on first run\n",
    "print(\"Loading YOLOv8 nano model for face detection...\")\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "print(\"âœ… Face detection model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b55d5a3",
   "metadata": {},
   "source": [
    "## 3. Face Alignment Functions\n",
    "\n",
    "Implement 5-point landmark alignment for face normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48779fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignment functions defined!\n"
     ]
    }
   ],
   "source": [
    "def align_face(img, landmarks, output_size=(112, 112)):\n",
    "    \"\"\"\n",
    "    Align face using 5-point landmarks (eyes, nose, mouth corners)\n",
    "    \n",
    "    Args:\n",
    "        img: Input image (BGR)\n",
    "        landmarks: 5x2 array of facial landmarks\n",
    "        output_size: Desired output size\n",
    "    \n",
    "    Returns:\n",
    "        Aligned and cropped face image\n",
    "    \"\"\"\n",
    "    # Standard 5-point template for 112x112 face\n",
    "    template = np.array([\n",
    "        [38.2946, 51.6963],  # Left eye\n",
    "        [73.5318, 51.5014],  # Right eye\n",
    "        [56.0252, 71.7366],  # Nose tip\n",
    "        [41.5493, 92.3655],  # Left mouth corner\n",
    "        [70.7299, 92.2041]   # Right mouth corner\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    # Estimate affine transform\n",
    "    tform = cv2.estimateAffinePartial2D(landmarks, template)[0]\n",
    "    \n",
    "    # Apply transformation\n",
    "    aligned = cv2.warpAffine(img, tform, output_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return aligned\n",
    "\n",
    "def normalize_face(img):\n",
    "    \"\"\"\n",
    "    Normalize face image for embedding extraction\n",
    "    \"\"\"\n",
    "    # Convert to RGB and normalize to [-1, 1]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = (img.astype(np.float32) - 127.5) / 128.0\n",
    "    return img\n",
    "\n",
    "print(\"Alignment functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3855df0c",
   "metadata": {},
   "source": [
    "## 4. Process Gallery Images\n",
    "\n",
    "Detect faces, extract landmarks, align, and save processed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea7415b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing gallery images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 3/20 [00:01<00:09,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple faces: Screenshot 2025-11-07 181438.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 181444.png (using largest)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:04<00:05,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple faces: Screenshot 2025-11-07 182041.png (using largest)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:05<00:04,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple faces: Screenshot 2025-11-07 182125.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 182130.png (using largest)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:05<00:04,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple faces: Screenshot 2025-11-07 182142.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 180438.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 180448.png (using largest)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:06<00:03,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple faces: Screenshot 2025-11-07 180505.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 182158.png (using largest)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:06<00:03,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple faces: Screenshot 2025-11-07 182209.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 182219.png (using largest)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:06<00:02,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No face detected: Screenshot 2025-10-30 113045.png\n",
      "Multiple faces: Screenshot 2025-11-07 180632.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 180632.png (using largest)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:08<00:02,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple faces: Screenshot 2025-11-07 181224.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 181233.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 181238.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 181233.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 181238.png (using largest)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:08<00:01,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple faces: Screenshot 2025-11-07 181242.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 181251.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 181255.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 181259.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 181255.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 181259.png (using largest)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:09<00:01,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple faces: Screenshot 2025-11-07 181317.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 181326.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 181330.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 181335.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 181330.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 181335.png (using largest)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:10<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple faces: Screenshot 2025-11-07 181339.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 180438.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 180448.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 180438.png (using largest)\n",
      "Multiple faces: Screenshot 2025-11-07 180448.png (using largest)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:10<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple faces: Screenshot 2025-11-07 180505.png (using largest)\n",
      "\n",
      "==================================================\n",
      "PROCESSING COMPLETE\n",
      "==================================================\n",
      "Total images: 99\n",
      "Successfully processed: 98\n",
      "No face detected: 1\n",
      "Multiple faces: 28\n",
      "Failed: 0\n",
      "Success rate: 99.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_gallery_images(input_dir, output_dir, model):\n",
    "    \"\"\"\n",
    "    Process all images in gallery: detect, crop, and save\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'total_images': 0,\n",
    "        'processed': 0,\n",
    "        'no_face': 0,\n",
    "        'multiple_faces': 0,\n",
    "        'failed': 0\n",
    "    }\n",
    "    \n",
    "    # Process each person's folder\n",
    "    for person_dir in tqdm(sorted(input_dir.iterdir())):\n",
    "        if not person_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        person_name = person_dir.name\n",
    "        output_person_dir = output_dir / person_name\n",
    "        output_person_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Process each image\n",
    "        for img_path in person_dir.glob('*'):\n",
    "            if img_path.suffix.lower() not in ['.jpg', '.jpeg', '.png']:\n",
    "                continue\n",
    "                \n",
    "            stats['total_images'] += 1\n",
    "            \n",
    "            try:\n",
    "                # Read image\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    stats['failed'] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Detect faces\n",
    "                results = model(img, verbose=False)\n",
    "                \n",
    "                if len(results[0].boxes) == 0:\n",
    "                    print(f\"No face detected: {img_path.name}\")\n",
    "                    stats['no_face'] += 1\n",
    "                    continue\n",
    "                    \n",
    "                if len(results[0].boxes) > 1:\n",
    "                    print(f\"Multiple faces: {img_path.name} (using largest)\")\n",
    "                    stats['multiple_faces'] += 1\n",
    "                \n",
    "                # Get bounding box of largest face\n",
    "                boxes = results[0].boxes\n",
    "                areas = (boxes.xyxy[:, 2] - boxes.xyxy[:, 0]) * (boxes.xyxy[:, 3] - boxes.xyxy[:, 1])\n",
    "                largest_idx = areas.argmax()\n",
    "                \n",
    "                # Get bounding box coordinates\n",
    "                x1, y1, x2, y2 = boxes.xyxy[largest_idx].cpu().numpy().astype(int)\n",
    "                \n",
    "                # Add padding around face (20% margin)\n",
    "                h, w = y2 - y1, x2 - x1\n",
    "                margin = max(h, w) * 0.2\n",
    "                x1 = max(0, int(x1 - margin))\n",
    "                y1 = max(0, int(y1 - margin))\n",
    "                x2 = min(img.shape[1], int(x2 + margin))\n",
    "                y2 = min(img.shape[0], int(y2 + margin))\n",
    "                \n",
    "                # Crop face\n",
    "                face = img[y1:y2, x1:x2]\n",
    "                \n",
    "                # Resize to 112x112 (standard face size)\n",
    "                face_resized = cv2.resize(face, (112, 112), interpolation=cv2.INTER_LINEAR)\n",
    "                \n",
    "                # Save\n",
    "                output_path = output_person_dir / img_path.name\n",
    "                cv2.imwrite(str(output_path), face_resized)\n",
    "                \n",
    "                stats['processed'] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path.name}: {e}\")\n",
    "                stats['failed'] += 1\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Run processing\n",
    "print(\"Processing gallery images...\")\n",
    "stats = process_gallery_images(GALLERY_DIR, ALIGNED_DIR, model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total images: {stats['total_images']}\")\n",
    "print(f\"Successfully processed: {stats['processed']}\")\n",
    "print(f\"No face detected: {stats['no_face']}\")\n",
    "print(f\"Multiple faces: {stats['multiple_faces']}\")\n",
    "print(f\"Failed: {stats['failed']}\")\n",
    "print(f\"Success rate: {stats['processed']/stats['total_images']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6785c13f",
   "metadata": {},
   "source": [
    "## 5. Create Train/Validation Split\n",
    "\n",
    "Reserve 1-2 images per person for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dba88ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… person1: 4 train, 1 val\n",
      "âœ… person10: 4 train, 1 val\n",
      "âœ… person11: 4 train, 1 val\n",
      "âœ… person12: 4 train, 1 val\n",
      "âœ… person13: 4 train, 1 val\n",
      "âœ… person14: 4 train, 1 val\n",
      "âœ… person15: 4 train, 1 val\n",
      "âœ… person16: 4 train, 1 val\n",
      "âœ… person17: 4 train, 1 val\n",
      "âœ… person18: 3 train, 1 val\n",
      "âœ… person19: 4 train, 1 val\n",
      "âœ… person2: 4 train, 1 val\n",
      "âœ… person20: 4 train, 1 val\n",
      "âœ… person3: 3 train, 1 val\n",
      "âœ… person4: 4 train, 1 val\n",
      "âœ… person5: 4 train, 1 val\n",
      "âœ… person6: 4 train, 1 val\n",
      "âœ… person7: 4 train, 1 val\n",
      "âœ… person8: 4 train, 1 val\n",
      "âœ… person9: 4 train, 1 val\n",
      "\n",
      "ðŸ“Š Split complete: 78 training, 20 validation images\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def create_validation_split(aligned_dir, validation_dir, val_images_per_person=1):\n",
    "    \"\"\"\n",
    "    Move some images to validation set\n",
    "    \"\"\"\n",
    "    total_train = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    for person_dir in aligned_dir.iterdir():\n",
    "        if not person_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        person_name = person_dir.name\n",
    "        val_person_dir = validation_dir / person_name\n",
    "        val_person_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Get all images\n",
    "        images = list(person_dir.glob('*.jpg')) + list(person_dir.glob('*.png'))\n",
    "        \n",
    "        if len(images) <= val_images_per_person:\n",
    "            print(f\"âš ï¸  {person_name}: only {len(images)} images (keeping all for training)\")\n",
    "            continue\n",
    "        \n",
    "        # Randomly select validation images\n",
    "        val_images = random.sample(images, val_images_per_person)\n",
    "        \n",
    "        # Move to validation\n",
    "        for img_path in val_images:\n",
    "            shutil.move(str(img_path), str(val_person_dir / img_path.name))\n",
    "        \n",
    "        train_count = len(images) - val_images_per_person\n",
    "        total_train += train_count\n",
    "        total_val += val_images_per_person\n",
    "        \n",
    "        print(f\"âœ… {person_name}: {train_count} train, {val_images_per_person} val\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Split complete: {total_train} training, {total_val} validation images\")\n",
    "\n",
    "# Create split (1 image per person for validation since we have limited images)\n",
    "create_validation_split(ALIGNED_DIR, VALIDATION_DIR, val_images_per_person=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2890dab9",
   "metadata": {},
   "source": [
    "## 6. Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f77b924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET STATISTICS\n",
      "============================================================\n",
      "Total identities: 20\n",
      "Total training images: 78\n",
      "Total validation images: 20\n",
      "Average images per person (train): 3.9\n",
      "Min images per person: 3\n",
      "Max images per person: 4\n"
     ]
    }
   ],
   "source": [
    "def dataset_statistics(aligned_dir, validation_dir):\n",
    "    \"\"\"\n",
    "    Generate dataset statistics\n",
    "    \"\"\"\n",
    "    train_stats = {}\n",
    "    val_stats = {}\n",
    "    \n",
    "    # Training set\n",
    "    for person_dir in aligned_dir.iterdir():\n",
    "        if person_dir.is_dir():\n",
    "            count = len(list(person_dir.glob('*.jpg'))) + len(list(person_dir.glob('*.png')))\n",
    "            train_stats[person_dir.name] = count\n",
    "    \n",
    "    # Validation set\n",
    "    for person_dir in validation_dir.iterdir():\n",
    "        if person_dir.is_dir():\n",
    "            count = len(list(person_dir.glob('*.jpg'))) + len(list(person_dir.glob('*.png')))\n",
    "            val_stats[person_dir.name] = count\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATASET STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total identities: {len(train_stats)}\")\n",
    "    print(f\"Total training images: {sum(train_stats.values())}\")\n",
    "    print(f\"Total validation images: {sum(val_stats.values())}\")\n",
    "    print(f\"Average images per person (train): {np.mean(list(train_stats.values())):.1f}\")\n",
    "    print(f\"Min images per person: {min(train_stats.values())}\")\n",
    "    print(f\"Max images per person: {max(train_stats.values())}\")\n",
    "    \n",
    "    return train_stats, val_stats\n",
    "\n",
    "train_stats, val_stats = dataset_statistics(ALIGNED_DIR, VALIDATION_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed30e4",
   "metadata": {},
   "source": [
    "## âœ… Summary\n",
    "\n",
    "**Task 1 Complete!**\n",
    "\n",
    "We have successfully:\n",
    "1. âœ… Organized gallery dataset (20 identities)\n",
    "2. âœ… Implemented face detection using YOLO\n",
    "3. âœ… Applied 5-point landmark alignment\n",
    "4. âœ… Normalized and cropped faces to 112x112\n",
    "5. âœ… Created train/validation split\n",
    "\n",
    "**Next Steps:** Task 2 - Face Detection Module with evaluation metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
